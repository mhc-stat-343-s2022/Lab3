---
title: "Lab 4: Posterior Exploration"
subtitle: "STAT 343: Mathematical Statistics"
output:
  pdf_document
---

\newcommand{\simiid}{{\mathrel {\mathop {\sim}\limits _{}^{\rm iid}}\,}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

## Introduction

Let's estimate the proportion of Starburst that are yellow.  Call this proportion $\theta$.  Suppose we take a sample of $n$ Starburst and let the random variable $X$ denote a count of how many are yellow in that sample.  Our model is 

$$X \sim \text{Binomial}(n, \theta).$$

We have developed two approaches to inference for $\theta$:

1. The maximum likelihood estimate $\hat{\theta}_{MLE} = \frac{x}{n}$, which maximizes the likelihood function $\mathcal{L}(\theta | x)$.

2. A Bayesian approach with conjugate prior distribution given by $\Theta \sim Beta(a, b)$.  The posterior distribution is given by $\Theta | n, x \sim Beta(a + x, b + n - x)$.  From this posterior distribution, we can obtain point estimates (such as the posterior mean, posterior median, posterior mode) and interval estimates (such as a posterior 95% credible interval - this is a first introduction; more on this when we get to confidence intervals/credible intervals later in the class).

We have three related goals in this lab:

1. To see what the posterior distribution looks like in Bayesian inference, and how this changes as the sample size $n$ increases.
2. To see what effect the prior distribution has on Bayesian inferences, and how this changes as the sample size $n$ increases.
3. To compare maximum likelihood and Bayesian estimates of $\theta$, and see how these estimates change as the sample size $n$ increases.

## Procedure

### Step 0. Prior Specifications

In order to get answers that are the same for everyone, and to understand the relationship between the prior distribution and the posterior, let's all work with the following three specifications of prior distributions:

 * Non-informative (Vague) Prior: a = 1 and b = 1
 * Informative Prior, weak prior knowledge: a = 2, b = 6
 * Informative Prior, strong prior knowledge: a = 20, b = 60

The pdfs for these three prior distributions are below:

```{r fig.align='center'}
a_noninformative <- 1
b_noninformative <- 1
a_weakly_informative <- 2
b_weakly_informative <- 6
a_strongly_informative <- 20
b_strongly_informative <- 60

ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  theme_bw()
```

The prior means and a 95% prior credible interval based on the non-informative prior are calculated below:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

The prior means and a 95% prior credible interval based on the weakly informative prior are calculated below:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

The prior means and a 95% prior credible interval based on the strongly informative prior are calculated below:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```

Make sure you understand what the different prior distributions say about the analyst's beliefs about the value of $\theta$ before moving on.

### Step 1. Take some samples and record data

In order to compare the estimates above, let's take samples of a few different sizes and plot the likelihood function, the posterior distribution based on two different prior distributions, and point estimates from each method for each sample size.

Take about 45 Starburst â€“ the exact number is not important.

Record the following:

Was your first Starburst yellow? (Pick a random Starburst from your sample that you will count as your first draw)

X=

Out of your first 10 Starburst, how many were yellow?

X=

Out of your first 20 Starburst, how many were yellow?

X=

Out of all of the Starburst you picked, how many were yellow?  Also, how big was your total sample size?

X = 

n = 

### Step 2. Find posterior distribution, point and interval estimates

#### Sample of size n = 1

The code below is exactly the same as the code above for exploring the prior distributions.  Update the code to explore the posterior distributions obtained from each prior specification based on the data observed for your sample of size 1, and compare to the maximum likelihood estimate.

You will have to make two changes:

1. Update the a and b parameters to be the posterior parameters corresponding to each prior.
2. Add in a vertical line at the maximum likelihood estimate (use `geom_vline(xintercept = *)`, replacing the * with the maximum likelihood estimate).

The rest of the code can be left as is.

```{r}
# Check Step 1 to see what I observed in my first draw
a_noninformative <- 1 #change me
b_noninformative <- 1 #change me
a_weakly_informative <- 2 #change me
b_weakly_informative <- 6 #change me
a_strongly_informative <- 20 #change me
b_strongly_informative <- 60 #change me

ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  theme_bw()
```

Prior mean and 95% posterior credible interval based on the non-informative prior:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

Prior mean and 95% posterior credible interval based on the weakly informative prior:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

Prior mean and a 95% posterior credible interval based on the strongly informative prior:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```


#### Sample of size n = 10

The code below is exactly the same as the code above for exploring the prior distributions.  Update the code to explore the posterior distributions obtained from each prior specification based on the data observed for your sample of size 10, and compare to the maximum likelihood estimate.

You will have to make two changes:

1. Update the a and b parameters to be the posterior parameters corresponding to each prior.
2. Add in a vertical line at the maximum likelihood estimate (use `geom_vline(xintercept = *)`, replacing the * with the maximum likelihood estimate).

The rest of the code can be left as is.

```{r}
# Check Step 1 to find out what I observed in my first 10 draws
a_noninformative <- 1
b_noninformative <- 1
a_weakly_informative <- 2
b_weakly_informative <- 6
a_strongly_informative <- 20
b_strongly_informative <- 60
ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  theme_bw()
```

Prior mean and 95% posterior credible interval based on the non-informative prior:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

Prior mean and 95% posterior credible interval based on the weakly informative prior:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

Prior mean and a 95% posterior credible interval based on the strongly informative prior:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```


#### Sample of size n = 20

The code below is exactly the same as the code above for exploring the prior distributions.  Update the code to explore the posterior distributions obtained from each prior specification based on the data observed for your sample of size 20, and compare to the maximum likelihood estimate.

You will have to make two changes:

1. Update the a and b parameters to be the posterior parameters corresponding to each prior.
2. Add in a vertical line at the maximum likelihood estimate (use `geom_vline(xintercept = *)`, replacing the * with the maximum likelihood estimate).

The rest of the code can be left as is.

```{r}
# Check step 1 to see what I observed in my first 20 draws
a_noninformative <- 1
b_noninformative <- 1
a_weakly_informative <- 2
b_weakly_informative <- 6
a_strongly_informative <- 20
b_strongly_informative <- 60
ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  theme_bw()
```

Prior mean and 95% posterior credible interval based on the non-informative prior:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

Prior mean and 95% posterior credible interval based on the weakly informative prior:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

Prior mean and a 95% posterior credible interval based on the strongly informative prior:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```


#### Sample of large size

The code below is exactly the same as the code above for exploring the prior distributions.  Update the code to explore the posterior distributions obtained from each prior specification based on the data observed for your large sample size, and compare to the maximum likelihood estimate.

You will have to make two changes:

1. Update the a and b parameters to be the posterior parameters corresponding to each prior.
2. Add in a vertical line at the maximum likelihood estimate (use `geom_vline(xintercept = *)`, replacing the * with the maximum likelihood estimate).

The rest of the code can be left as is.

```{r}
# Check Step 1 to see how many yellow starburst observed in a sample of size 46
a_noninformative <- 1
b_noninformative <- 1 
a_weakly_informative <- 2
b_weakly_informative <- 6
a_strongly_informative <- 20
b_strongly_informative <- 60
ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  theme_bw()
```

Prior mean and 95% posterior credible interval based on the non-informative prior:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

Prior mean and 95% posterior credible interval based on the weakly informative prior:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

Prior mean and a 95% posterior credible interval based on the strongly informative prior:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```



#### Combined sample across all groups in class

The code below is exactly the same as the code above for exploring the prior distributions.  Update the code to explore the posterior distributions obtained from each prior specification based on the data observed for your large sample size, and compare to the maximum likelihood estimate.

You will have to make two changes:

1. Update the a and b parameters to be the posterior parameters corresponding to each prior.
2. Add in a vertical line at the maximum likelihood estimate (use `geom_vline(xintercept = *)`, replacing the * with the maximum likelihood estimate).

The rest of the code can be left as is.

```{r}
# We had the following overall count of yellow starburst and sample size:

```

```{r}
a_noninformative <- 1
b_noninformative <- 1
a_weakly_informative <- 2
b_weakly_informative <- 6
a_strongly_informative <- 20
b_strongly_informative <- 60
ggplot(data = data.frame(theta = c(0, 1)), mapping = aes(x = theta)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_noninformative, shape2 = b_noninformative)) +
  stat_function(fun = dbeta,
    args = list(shape1 = a_weakly_informative, shape2 = b_weakly_informative),
    color = "cornflowerblue") +
  stat_function(fun = dbeta,
    args = list(shape1 = a_strongly_informative, shape2 = b_strongly_informative),
    color = "orange") +
  xlab(expression(theta)) + 
  ## add geom_vline
  theme_bw()
```

Prior mean and 95% posterior credible interval based on the non-informative prior:

```{r}
a_noninformative/(a_noninformative + b_noninformative)
qbeta(c(0.025, 0.975), shape1 = a_noninformative, shape2 = b_noninformative)
```

Prior mean and 95% posterior credible interval based on the weakly informative prior:

```{r}
a_weakly_informative/(a_weakly_informative + b_weakly_informative)
qbeta(c(0.025, 0.975), shape1 = a_weakly_informative, shape2 = b_weakly_informative)
```

Prior mean and a 95% posterior credible interval based on the strongly informative prior:

```{r}
a_strongly_informative/(a_strongly_informative + b_strongly_informative)
qbeta(c(0.025, 0.975), shape1 = a_strongly_informative, shape2 = b_strongly_informative)
```


#### Understanding what you saw above

Pick one sample size (maybe 10).  How do the three posterior distributions for $\theta$ compare?  How does the strength of prior knowledge relate to the strength of posterior knowledge?  How do the three point estimates and the credible intervals compare?  Are some intervals wider or narrower than others?


Focus now on how the posterior distribution coming out of the analysis using a weakly informative prior distribution changes as the sample size increases.  How do the posterior means compare to the maximum likelihood estimate?  How does the width of the posterior credible interval change as the sample size increases?

